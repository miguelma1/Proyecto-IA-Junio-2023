{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leemos los ficheros iniciales de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from statistics import mode\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcosData = pd.read_csv('./datos/pcos.csv', skiprows = 1, header = None,\n",
    "                           names=['Age (yrs)', 'Weight (Kg)', 'Height(Cm)', 'BMI', 'Blood Group', 'Pulse rate(bpm)',\n",
    "                                   'RR (breaths/min)', 'Hb(g/dl)', 'Cycle(R/I)', 'Cycle length(days)', 'Marriage Status (Yrs)',\n",
    "                                     'Pregnant(Y/N)', 'No. of abortions', 'I beta-HCG(mIU/mL)', 'FSH(mIU/mL)', 'LH(mIU/mL)', 'FSH/LH', 'Hip(inch)',\n",
    "                                       'Waist(inch)', 'Waist:Hip Ratio', 'TSH (mIU/L)', 'PRL(ng/mL)', 'Vit D3 (ng/mL)', 'PRG(ng/mL)', 'RBS(mg/dl)', 'Weight gain(Y/N)', 'hair growth(Y/N)',\n",
    "                                         'Skin darkening (Y/N)', 'Hair loss(Y/N)', 'Pimples(Y/N)', 'Fast food (Y/N)', 'Reg.Exercise(Y/N)', 'BP _Systolic (mmHg)', 'BP _Diastolic (mmHg)', 'Follicle No. (L)',\n",
    "                                           'Follicle No. (R)', 'Avg. F size (L) (mm)', 'Avg. F size (R) (mm)', 'Endometrium (mm)', 'PCOS (Y/N)'])\n",
    "\n",
    "\n",
    "titanicData = pd.read_csv('./datos/titanic.csv', skiprows = 1, header = None,\n",
    "                              names=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Initial', 'Age_band', 'Family_Size', 'Alone', 'Fare_cat', 'Deck', 'Title', 'Is_Married', 'Survived'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la funcion para estandarizar los ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataStandarization (data, dataName, excludedColumns):\n",
    "    if not isinstance (dataName, str):\n",
    "        print(\"El argumento dataName debe ser una cadena de carateres.\")\n",
    "    \n",
    "    elif not isinstance (excludedColumns, list):\n",
    "        print(\"El argumento excludedColumns debe ser una lista con el nombre de las columnas a excluir.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        data_copy = data.drop(excludedColumns, axis = 1)\n",
    "        numerical_columns = [col for col in data.columns if col not in excludedColumns]\n",
    "        columns_to_standarize = data[numerical_columns]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        columns_to_standarize = scaler.fit_transform(columns_to_standarize)\n",
    "        data[numerical_columns] = columns_to_standarize\n",
    "\n",
    "        data.to_csv('./datos/' + dataName + '_standarized.csv', index = False)\n",
    "\n",
    "        print(\"El nuevo fichero estandarizado se ha guardado en el directorio datos con el nombre \" + dataName + \".csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las columnas que no vamos a considerar para la estandarización, en este caso las columnas booleanas y las columnas resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nuevo fichero estandarizado se ha guardado en el directorio datos con el nombre titanic.csv\n",
      "El nuevo fichero estandarizado se ha guardado en el directorio datos con el nombre pcos.csv\n"
     ]
    }
   ],
   "source": [
    "titanic_excluded = ['Sex', 'Alone', 'Is_Married', 'Survived']\n",
    "pcos_excluded = ['Pregnant(Y/N)', 'Weight gain(Y/N)', 'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)', 'Pimples(Y/N)', 'Fast food (Y/N)','Reg.Exercise(Y/N)', 'PCOS (Y/N)']\n",
    "\n",
    "dataStandarization(titanicData, 'titanic', titanic_excluded)\n",
    "dataStandarization(pcosData, 'pcos', pcos_excluded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación dividiremos los ficheros estandarizados en dos, uno para el conjunto de entrenamiento y otro para el conjunto de prueba. Para ello definiremos la función necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit (data, dataName):\n",
    "    if not isinstance (dataName, str):\n",
    "        print(\"El argumento dataName debe ser una cadena de carateres.\")\n",
    "    \n",
    "    else:\n",
    "        data_train, data_test = model_selection.train_test_split(data, test_size = 0.3, random_state = 99)\n",
    "        data_train.to_csv('./datos/' + dataName + '_train.csv', index = False)\n",
    "        data_test.to_csv('./datos/' + dataName + '_test.csv', index = False)\n",
    "\n",
    "        print('Las dimensiones originales de los datos de entrada son: ', data.shape)\n",
    "        print('El conjunto de entrenamiento se ha guardado en el directorio datos con el nombre ' + dataName + '_train.csv, y sus dimensiones son: ', data_train.shape)\n",
    "        print('El conjunto de pruebas se ha guardado en el directorio datos con el nombre ' + dataName + '_test.csv, y sus dimensiones son: ', data_test.shape)\n",
    "        return data_train, data_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente creamos los dos conjuntos de datos a partir del fichero estandarizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las dimensiones originales de los datos de entrada son:  (891, 16)\n",
      "El conjunto de entrenamiento se ha guardado en el directorio datos con el nombre titanic_train.csv, y sus dimensiones son:  (623, 16)\n",
      "El conjunto de pruebas se ha guardado en el directorio datos con el nombre titanic_test.csv, y sus dimensiones son:  (268, 16)\n",
      "Las dimensiones originales de los datos de entrada son:  (541, 40)\n",
      "El conjunto de entrenamiento se ha guardado en el directorio datos con el nombre pcos_train.csv, y sus dimensiones son:  (378, 40)\n",
      "El conjunto de pruebas se ha guardado en el directorio datos con el nombre pcos_test.csv, y sus dimensiones son:  (163, 40)\n"
     ]
    }
   ],
   "source": [
    "titanicTrain, titanicTest = dataSplit(titanicData, 'titanic')\n",
    "pcosTrain, pcosTest = dataSplit(pcosData, 'pcos')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez estandarizados los ficheros de entrenamientos, definiremos una función para que, dado un conjunto de datos, se generen diferentes conjuntos de datos de entrenamiento aplicando primero la técnica de Bootstrapping y más adelante Rndom Subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generadorConjutosEntrenamiento(fileName, data, amountFiles):\n",
    "\n",
    "    res= [] #lista de conjuntos de entrenamientos\n",
    "\n",
    "    col = data[data.columns[:-1]]# cogemos todas las columnas salvo la ultima \n",
    "    \n",
    "    objectiveVariable = data.loc[:, data.columns == data.columns[-1]] #guardamos la ultima columna\n",
    "\n",
    "    numCol=col.shape[1] #dimension de la columna en col\n",
    "\n",
    "    #hacemos bootstrapping\n",
    "    for i in range (amountFiles):\n",
    "        bootstrap_sample = data.sample(frac = 0.8, replace = True) \n",
    "        #para cada subset consideramos solo la mitad de los datos de entrada y con posibilidad de repeticion\n",
    "        for j in range (amountFiles): #para cada nuevo archivo bootstrappeado\n",
    "            #cogemos una serie de columnas aleatorias    \n",
    "            selcted_column=np.random.choice(numCol,size=int(np.sqrt(numCol)),replace=False)#seleccionamos un subconjunto de columnas \n",
    "            subspace_sample = col.iloc[:, selcted_column].copy()#creamos un nuevo DataSet \n",
    "            #-----------------------------A LO MEJOR HAY QUE CAMBIARLO---------------\n",
    "            subspace_sample[data.columns.values[-1]] = objectiveVariable #añadimos columna survived\n",
    "            #------------------------------------------------------------------------\n",
    "\n",
    "            res.append(subspace_sample)\n",
    "\n",
    "            route = f'./datos/conjuntosEntrenamiento/{fileName}_trainSet_{i+1}.{j+1}.csv'\n",
    "            subspace_sample.to_csv(route, index = False)\n",
    "            print('Fichero creados en la ruta: ', route)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definiremos la función encargada del entrenamiento de modelos, la cual llamará a la función generadora definida previamente. Para ello definiremos previamente una función que nos será útil para dividir las variables objetivos del resto de variables.\n",
    "\n",
    "También definiremos dos funciones, una que dada una lista de listas de valores, nos devuelva una lista con las modas de los valores de la misma posición, y otra que dado una lista de modelos entrenados y un conjuto de datos, nos prediga el valor de la variable objetivo utilizando la moda de los resultdos obtenidos en cada uno de los modelos.\n",
    "\n",
    "A continuación definiremos las siguientes funciones:\n",
    "- separarVariables: Útil para dividir las variables objetivos del resto de variables.\n",
    "- entrenamientoDeModelos: Algoritmo encargado de entrenar una serie de modelos a partir de un conjunto de entrenamiento, para ello necesitara diferentes argumentos:\n",
    "    - data: Conjunto de datos de entrenamiento.\n",
    "    - numModelos: Numero de modelos a entrenar, debido a als técnicas aplicadas, el resultado final de modelos es igual a numModelos * numModelos.\n",
    "    - algoritmo: Argumento de tipo String, de valor \"TREE\" o \"SGD\" en función del algoritmo deseado para el entrenamiento de los modelos.\n",
    "    - proporcionColumnas: Numero entre 0 y 1 que representa el porcentaje de columnas empleadas para el entrenamiento de los modelos.\n",
    "    - fileName: String que representa el nombre del archivo, para seguir la coherencia del proyecto, esta será \"titanic\" o \"pcos\" en función del conjunto de datos.\n",
    "- modasLista: Dada una lista de listas de numeros, devuelve una lista con las modas de los numeros de una misma posición.\n",
    "- algoritmoPrediccion: A partir de unos datos de pruebas y una lista de modelos, predice el valor de la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separarVariables(data):\n",
    "    x = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def entrenamientoDeModelos(data, numModelos, algoritmo, proporcionColumnas, fileName):\n",
    "    res = []\n",
    "# si no esta entre 0 y 1 paro\n",
    "    if not 0 <= proporcionColumnas <= 1:\n",
    "        print(\"El parametro proporcionColumnas debe ser un numero entre 0 y 1\")\n",
    "\n",
    "    else:\n",
    "    #si el argumento no es un string\n",
    "        if not isinstance(algoritmo, str): \n",
    "            print(\"El argumento algoritmo no es un String\")\n",
    "    #si es un string\n",
    "        else:\n",
    "            #dado el dataset original obtengo una lista de datos de entrenamiento (hay que ver el numModelos)\n",
    "            training_data = generadorConjutosEntrenamiento(fileName, data, numModelos)\n",
    "#por cada dato\n",
    "            for i in training_data:\n",
    "                #vego que algoritmo se solicita y creo una nueva instancia del mismo\n",
    "                if algoritmo.upper() == 'TREE':\n",
    "                    alg = DecisionTreeClassifier()\n",
    "        \n",
    "                elif  algoritmo.upper() == 'SGD':\n",
    "                    alg = SGDClassifier()\n",
    "    #separo las variables de datos de las objetivos y ajusto entreno el algoritmo\n",
    "                x, y = separarVariables(i)\n",
    "                alg.fit(x, y)\n",
    "#meto en una lista el modelo entrenado y las columnas empleadas en su entrenamiento\n",
    "                res.append((alg,x.columns))\n",
    "    return res\n",
    "\n",
    "def modasLista(list_of_lists):\n",
    "    result = []\n",
    "    list_length = len(list_of_lists[0])\n",
    "\n",
    "    for i in range(list_length):\n",
    "        elements = [lst[i] for lst in list_of_lists]    \n",
    "        result.append(mode(elements))\n",
    "\n",
    "    return result\n",
    "\n",
    "def algoritmoPrediccion(datosTesteo, conjunto):\n",
    "    ls = []\n",
    "    for i in conjunto:\n",
    "        pred = i[0].predict(datosTesteo[i[1]])\n",
    "        ls.append(pred)\n",
    "    return  modasLista(ls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente predecimos los resultados al pasarle un conjunto de datos de pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_1.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_1.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_1.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_2.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_2.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_2.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_3.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_3.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/titanic_trainSet_3.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_1.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_1.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_1.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_1.4.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_2.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_2.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_2.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_2.4.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_3.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_3.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_3.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_3.4.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_4.1.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_4.2.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_4.3.csv\n",
      "Fichero creados en la ruta:  ./datos/conjuntosEntrenamiento/pcos_trainSet_4.4.csv\n"
     ]
    }
   ],
   "source": [
    "titanicModels = entrenamientoDeModelos(titanicTrain, 3, 'sgd', 0.5, 'titanic')\n",
    "pcosModels = entrenamientoDeModelos(pcosTrain, 4, 'tree', 0.5, 'pcos')\n",
    "\n",
    "titanicPredictions = algoritmoPrediccion(titanicTest, titanicModels)\n",
    "pcosPredictions = algoritmoPrediccion(pcosTest, pcosModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def balancedAccuracyScore(testingData, predicted):\n",
    "    return balanced_accuracy_score(testingData.iloc[:,-1].tolist(), predicted)\n",
    "\n",
    "def f1Score(testingData, predicted):\n",
    "    return f1_score(testingData.iloc[:,-1].tolist(), predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300684764000979\n",
      "0.7311957569913211\n",
      "0.6486486486486486\n",
      "0.6382978723404255\n"
     ]
    }
   ],
   "source": [
    "print(balancedAccuracyScore(titanicTest, titanicPredictions))\n",
    "print(balancedAccuracyScore(pcosTest, pcosPredictions))\n",
    "\n",
    "print(f1Score(titanicTest, titanicPredictions))\n",
    "print(f1Score(pcosTest, pcosPredictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
